"use strict";(self.webpackChunknocobase_docs=self.webpackChunknocobase_docs||[]).push([[53994],{184303:function(d,t,n){n.r(t);var a=n(572269),_=n(793359),u=n(861788),h=n(719977),m=n(20190),x=n(24268),p=n(496057),v=n(585939),I=n(28484),j=n(635206),f=n(375553),E=n(156266),M=n(572333),i=n(841118),g=n(39297),L=n(868526),O=n(605019),o=n(614651),r=n(280936),l=n(667294),s=n(851833),e=n(785893);function c(){return(0,e.jsx)(o.dY,{children:(0,e.jsx)(l.Suspense,{fallback:(0,e.jsx)(r.Z,{}),children:(0,e.jsxs)(e.Fragment,{children:[(0,e.jsx)("div",{className:"markdown",children:(0,e.jsxs)("h1",{id:"chat",children:[(0,e.jsx)("a",{"aria-hidden":"true",tabIndex:"-1",href:"#chat",children:(0,e.jsx)("span",{className:"icon icon-link"})}),"Chat"]})}),(0,e.jsx)(i.Z,{name:"ai"}),(0,e.jsxs)("div",{className:"markdown",children:[(0,e.jsxs)("h2",{id:"introduction",children:[(0,e.jsx)("a",{"aria-hidden":"true",tabIndex:"-1",href:"#introduction",children:(0,e.jsx)("span",{className:"icon icon-link"})}),"Introduction"]}),(0,e.jsx)("p",{children:s.texts[0].value}),(0,e.jsx)("p",{children:(0,e.jsx)("img",{src:"https://static-docs.nocobase.com/202503041012091.png",alt:""})}),(0,e.jsxs)("h2",{id:"creating-a-new-llm-node",children:[(0,e.jsx)("a",{"aria-hidden":"true",tabIndex:"-1",href:"#creating-a-new-llm-node",children:(0,e.jsx)("span",{className:"icon icon-link"})}),"Creating a New LLM Node"]}),(0,e.jsx)("p",{children:s.texts[1].value}),(0,e.jsx)("p",{children:(0,e.jsx)("img",{src:"https://static-docs.nocobase.com/202503041013363.png",alt:""})}),(0,e.jsxs)("h2",{id:"selecting-a-model",children:[(0,e.jsx)("a",{"aria-hidden":"true",tabIndex:"-1",href:"#selecting-a-model",children:(0,e.jsx)("span",{className:"icon icon-link"})}),"Selecting a Model"]}),(0,e.jsxs)("p",{children:[s.texts[2].value,(0,e.jsx)(o.rU,{to:"/handbook/ai/service",children:s.texts[3].value})]}),(0,e.jsx)("p",{children:s.texts[4].value}),(0,e.jsx)("p",{children:(0,e.jsx)("img",{src:"https://static-docs.nocobase.com/202503041013084.png",alt:""})}),(0,e.jsxs)("h2",{id:"configuring-call-parameters",children:[(0,e.jsx)("a",{"aria-hidden":"true",tabIndex:"-1",href:"#configuring-call-parameters",children:(0,e.jsx)("span",{className:"icon icon-link"})}),"Configuring Call Parameters"]}),(0,e.jsx)("p",{children:s.texts[5].value}),(0,e.jsx)("p",{children:(0,e.jsx)("img",{src:"https://static-docs.nocobase.com/202503041014778.png",alt:""})}),(0,e.jsxs)("h3",{id:"response-format",children:[(0,e.jsx)("a",{"aria-hidden":"true",tabIndex:"-1",href:"#response-format",children:(0,e.jsx)("span",{className:"icon icon-link"})}),"Response Format"]}),(0,e.jsxs)("p",{children:[s.texts[6].value,(0,e.jsx)("strong",{children:s.texts[7].value}),s.texts[8].value]}),(0,e.jsxs)("ul",{children:[(0,e.jsxs)("li",{children:[s.texts[9].value,(0,e.jsx)("code",{children:s.texts[10].value}),s.texts[11].value,(0,e.jsx)("code",{children:s.texts[12].value}),s.texts[13].value,(0,e.jsx)("code",{children:s.texts[14].value}),s.texts[15].value]}),(0,e.jsxs)("li",{children:[s.texts[16].value,(0,e.jsx)(o.rU,{to:"/handbook/ai-ee/workflow/nodes/llm/structured-output",children:s.texts[17].value}),s.texts[18].value]})]}),(0,e.jsxs)("h2",{id:"messages",children:[(0,e.jsx)("a",{"aria-hidden":"true",tabIndex:"-1",href:"#messages",children:(0,e.jsx)("span",{className:"icon icon-link"})}),"Messages"]}),(0,e.jsx)("p",{children:s.texts[19].value}),(0,e.jsxs)("ul",{children:[(0,e.jsx)("li",{children:s.texts[20].value}),(0,e.jsx)("li",{children:s.texts[21].value}),(0,e.jsx)("li",{children:s.texts[22].value})]}),(0,e.jsxs)("p",{children:[s.texts[23].value,(0,e.jsx)("code",{children:s.texts[24].value}),s.texts[25].value,(0,e.jsx)("code",{children:s.texts[26].value}),s.texts[27].value]}),(0,e.jsx)("p",{children:(0,e.jsx)("img",{src:"https://static-docs.nocobase.com/202503041016140.png",alt:""})}),(0,e.jsx)("p",{children:s.texts[28].value}),(0,e.jsx)("p",{children:(0,e.jsx)("img",{src:"https://static-docs.nocobase.com/202503041017879.png",alt:""})}),(0,e.jsxs)("h2",{id:"using-the-response-content-of-the-llm-node",children:[(0,e.jsx)("a",{"aria-hidden":"true",tabIndex:"-1",href:"#using-the-response-content-of-the-llm-node",children:(0,e.jsx)("span",{className:"icon icon-link"})}),"Using the Response Content of the LLM Node"]}),(0,e.jsx)("p",{children:s.texts[29].value}),(0,e.jsx)("p",{children:(0,e.jsx)("img",{src:"https://static-docs.nocobase.com/202503041018508.png",alt:""})})]})]})})})}t.default=c},851833:function(d,t,n){n.r(t),n.d(t,{texts:function(){return a}});const a=[{value:"The LLM node in the workflow can initiate conversations with online LLM services, leveraging the capabilities of powerful models to assist in completing a series of business processes.",paraId:0,tocIndex:1},{value:"Since conversing with LLM services is typically time-consuming, the LLM node can only be used in asynchronous workflows.",paraId:1,tocIndex:2},{value:"First, choose an integrated LLM service. If an LLM service has not been integrated yet, you need to add an LLM service configuration first. For reference, see: ",paraId:2,tocIndex:3},{value:"LLM Service Management",paraId:3,tocIndex:3},{value:"After selecting the service, the application will attempt to fetch a list of available models from the LLM service for your selection. Some online LLM services may provide model-fetching APIs that do not conform to the standard API protocol, in which case you can manually input the model ID.",paraId:4,tocIndex:3},{value:"You can adjust the parameters for calling the LLM model as needed.",paraId:5,tocIndex:4},{value:"Note the ",paraId:6,tocIndex:5},{value:"Response Format",paraId:6,tocIndex:5},{value:" setting, which is used to indicate the format of the response from the model, either text or JSON. If you choose JSON mode, note the following:",paraId:6,tocIndex:5},{value:'The corresponding LLM model must support calls in JSON mode, and you must explicitly prompt the model to respond in JSON format in your prompt. For example: "Tell me a joke about cats, respond in JSON with ',paraId:7,tocIndex:5},{value:"setup",paraId:7,tocIndex:5},{value:" and ",paraId:7,tocIndex:5},{value:"punchline",paraId:7,tocIndex:5},{value:' keys." Otherwise, there may be no response and it could result in a ',paraId:7,tocIndex:5},{value:"400 status code (no body)",paraId:7,tocIndex:5},{value:" error.",paraId:7,tocIndex:5},{value:"The response will be a JSON string, so you need to use other workflow nodes to parse it in order to use the structured content. You can also use the ",paraId:7,tocIndex:5},{value:"Structured Output",paraId:8,tocIndex:5},{value:" feature.",paraId:7,tocIndex:5},{value:"The array of messages sent to the LLM model can include a history of conversations. The messages support three types:",paraId:9,tocIndex:6},{value:"System \u2013 Typically used to define the role and behavior the LLM model should play in the conversation.",paraId:10,tocIndex:6},{value:"User \u2013 Content entered by the user.",paraId:10,tocIndex:6},{value:"Assistant \u2013 Content that is the model's response.",paraId:10,tocIndex:6},{value:"For user messages, if supported by the model, you can combine multiple pieces of content in one prompt corresponding to the ",paraId:11,tocIndex:6},{value:"content",paraId:11,tocIndex:6},{value:" parameter. If the model only supports a string format for the ",paraId:11,tocIndex:6},{value:"content",paraId:11,tocIndex:6},{value:" parameter (which is the case for most models that do not support multimodal conversations), please split the messages into multiple prompts, each containing only one piece of content. This way, the node will send the content as a string.",paraId:11,tocIndex:6},{value:"You can use variables in the message content to reference the workflow's context.",paraId:12,tocIndex:6},{value:"The response from the LLM node can be used as a variable in other nodes.",paraId:13,tocIndex:7}]}}]);
