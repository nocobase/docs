# Чат с ИИ

<PluginInfo name="ai"></PluginInfo>

## Введение

Узел LLM в workflow позволяет взаимодействовать с онлайн-сервисами языковых моделей, используя их возможности для автоматизации бизнес-процессов.

![](https://static-docs.nocobase.com/202503041012091.png)

## Создание узла LLM

Поскольку взаимодействие с LLM-сервисами обычно требует времени, узел LLM можно использовать только в асинхронных workflow.

![](https://static-docs.nocobase.com/202503041013363.png)

## Выбор модели

1. Сначала выберите интегрированный LLM-сервис. Если сервис ещё не подключён, необходимо сначала добавить его конфигурацию (см. [Управление LLM-сервисами](./../../../service.md))

2. После выбора сервиса система попытается получить список доступных моделей. Для некоторых сервисов может потребоваться ввести ID модели вручную.

![](https://static-docs.nocobase.com/202503041013084.png)

## Настройка параметров вызова

Вы можете настроить параметры вызова модели под свои задачи.

![](https://static-docs.nocobase.com/202503041014778.png)

### Формат ответа

Особое внимание уделите настройке **Формата ответа** (текст или JSON). При выборе JSON:

- Модель должна поддерживать JSON-режим
- В запросе нужно явно указать требование отвечать в JSON-формате
- Ответ будет в виде JSON-строки, требующей дополнительного парсинга
- Можно использовать функцию [Структурированный вывод](../../../../../handbook/ai-ee/workflow/nodes/llm/structured-output)

## Сообщения

Массив сообщений для LLM может включать историю диалога. Поддерживаются 3 типа сообщений:

1. **Системные** - определяют роль и поведение модели
2. **Пользовательские** - ввод пользователя 
3. **Ассистента** - ответы модели

Для пользовательских сообщений можно комбинировать несколько элементов контента в одном запросе (если модель поддерживает). В противном случае - разделяйте на отдельные сообщения.

![](https://static-docs.nocobase.com/202503041016140.png)

В содержимом сообщений можно использовать переменные из контекста workflow.

![](https://static-docs.nocobase.com/202503041017879.png)

## Использование ответа LLM

Ответ узла LLM можно использовать как переменную в других узлах workflow.

![](https://static-docs.nocobase.com/202503041018508.png)

